# PANOSETI PFF to Zarr Conversion Configuration
# HIGH PERFORMANCE PROFILE WITH DASK DISTRIBUTED COMPUTING
# 
# Optimized for:
# - Multi-node distributed processing via Dask
# - Maximum throughput on BeeGFS/HDD
# - High CPU core utilization across cluster
# - Good compression ratio (blosc-lz4 level 5)

[pff_to_zarr]

# Compression Settings
codec = "blosc-lz4"
level = 5

# Zarr Chunking
time_chunk = 65536

# Dask Distributed Computing
use_dask = true  # Set to false to use local multiprocessing instead
dask_scheduler_address = ""  # Empty = create SSH cluster, or provide address like "tcp://10.0.1.2:8786"

# SSH Cluster Configuration (used when dask_scheduler_address is empty)
ssh_hosts = ["localhost", "panoseti-dfs0", "panoseti-dfs1", "panoseti-dfs2"]  # List of SSH hostnames
ssh_workers_per_host = 1  # Number of workers per host
ssh_threads_per_worker = 16  # Threads per worker
ssh_memory_per_worker = "16GB"  # Memory limit per worker

# Local Multiprocessing (used when use_dask = false)
num_workers = 10
blosc_threads = 8
max_concurrent_writes = 16
chunk_size_mb = 150

[baseline_subtract]

# Step 2 (L0 -> L1) Configuration

baseline_window = 100

# Compression Settings
codec = "blosc-lz4"
level = 5

# Dask Distributed Computing
use_dask = true
dask_scheduler_address = ""  # Empty = reuse cluster from step 1

# Computation Settings
compute_chunk_size = 8192  # Frames to process at once
