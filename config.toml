# PANOSETI PFF to Zarr Conversion Configuration
# Cluster-agnostic configuration with extensible cluster types

[cluster]
# Cluster type determines which backend to use
# Supported types: "ssh", "local"
type = "local"

# Cluster state management
use_cluster = false  # Set to true to enable distributed computing

[cluster.ssh]
# SSH Cluster Configuration (dask.distributed.SSHCluster)
hosts = ["localhost", "panoseti-dfs0", "panoseti-dfs1", "panoseti-dfs2"]
workers_per_host = 1
threads_per_worker = 16
memory_per_worker = "32GB"
scheduler_port = 0  # 0 = auto-assign
dashboard_port = 8797
connect_timeout = 60  # seconds to wait for workers

[cluster.local]
# Local Cluster Configuration (dask.distributed.LocalCluster)
n_workers = 4
threads_per_worker = 2
memory_per_worker = "4GB"
dashboard_port = 8787

#[cluster.slurm]
## SLURM Cluster Configuration (dask_jobqueue.SLURMCluster)
#queue = "normal"
#account = "myaccount"
#cores = 16
#memory = "32GB"
#walltime = "04:00:00"
#n_workers = 4
#job_extra_directives = []
#
#[cluster.kubernetes]
## Kubernetes Cluster Configuration (dask_kubernetes.KubeCluster)
#namespace = "dask"
#image = "daskdev/dask:latest"
#n_workers = 4
#memory_limit = "8GB"
#memory_request = "4GB"
#cpu_limit = 2.0
#cpu_request = 1.0

#[cluster.custom]
#your_param = "value"
#another_param = 123

[pff_to_zarr]
# Compression Settings
codec = "zstd"
level = 5
time_chunk = 32768
num_workers = 5
blosc_threads = 10
max_concurrent_writes = 16
chunk_size_mb = 200

[baseline_subtract]
baseline_window = 100
codec = "zstd"
level = 5
compute_chunk_size = 16384